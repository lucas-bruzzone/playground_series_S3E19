{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom colorama import Style, Fore\nblk = Style.BRIGHT + Fore.BLACK\nred = Style.BRIGHT + Fore.RED\nblu = Style.BRIGHT + Fore.BLUE\nres = Style.RESET_ALL\n\nimport os\ntrain = pd.read_csv('/kaggle/input/playground-series-s3e19/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s3e19/test.csv')\ntest['num_sold'] = float('nan') # this is dummy for simplify concat\n\ntrain.columns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-14T20:11:06.664111Z","iopub.execute_input":"2023-07-14T20:11:06.664510Z","iopub.status.idle":"2023-07-14T20:11:08.286390Z","shell.execute_reply.started":"2023-07-14T20:11:06.664479Z","shell.execute_reply":"2023-07-14T20:11:08.285068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analysis = train\nuniques = {}\n\nfor column in analysis.columns:\n    uniques[column] = analysis[column].unique().tolist()\n    if column not in ['date', 'num_sold','id']:\n        print(uniques[column])","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:11:08.288153Z","iopub.execute_input":"2023-07-14T20:11:08.288512Z","iopub.status.idle":"2023-07-14T20:11:08.370158Z","shell.execute_reply.started":"2023-07-14T20:11:08.288481Z","shell.execute_reply":"2023-07-14T20:11:08.368955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To Datetime\ntrain['date'] = pd.to_datetime(train['date'])\ntest['date'] = pd.to_datetime(test['date'])\n\ndf = pd.concat([train, test], axis=0)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:11:08.371759Z","iopub.execute_input":"2023-07-14T20:11:08.372153Z","iopub.status.idle":"2023-07-14T20:11:08.439119Z","shell.execute_reply.started":"2023-07-14T20:11:08.372119Z","shell.execute_reply":"2023-07-14T20:11:08.437913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\ncols = ['country', 'store', 'product']\nfor col in cols:\n    df[col] = le.fit_transform(df[col])","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:11:08.442120Z","iopub.execute_input":"2023-07-14T20:11:08.442514Z","iopub.status.idle":"2023-07-14T20:11:08.621572Z","shell.execute_reply.started":"2023-07-14T20:11:08.442479Z","shell.execute_reply":"2023-07-14T20:11:08.620390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime as dt\n\n# date column is separated for each element\ndf['day']   = df['date'].dt.day\ndf['week' ] = df['date'].dt.dayofweek\ndf['month'] = df['date'].dt.month\ndf['year']  = df['date'].dt.year","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:11:08.623479Z","iopub.execute_input":"2023-07-14T20:11:08.623856Z","iopub.status.idle":"2023-07-14T20:11:08.704547Z","shell.execute_reply.started":"2023-07-14T20:11:08.623822Z","shell.execute_reply":"2023-07-14T20:11:08.703360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:11:08.706208Z","iopub.execute_input":"2023-07-14T20:11:08.706907Z","iopub.status.idle":"2023-07-14T20:11:08.727401Z","shell.execute_reply.started":"2023-07-14T20:11:08.706872Z","shell.execute_reply":"2023-07-14T20:11:08.726224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seasonality_features(df_temp):\n    df_copy = df_temp.copy()  # Criar uma cópia do DataFrame\n    df_copy['month_sin'] = np.sin(2*np.pi*df_copy.month/12)\n    df_copy['month_cos'] = np.cos(2*np.pi*df_copy.month/12)\n    df_copy['day_sin'] = np.sin(2*np.pi*df_copy.day/24)\n    df_copy['day_cos'] = np.cos(2*np.pi*df_copy.day/24)\n    return df_copy\n\ndf_modified = seasonality_features(df)\n\ndf_modified = df_modified.drop('date', axis=1)\ndf_modified = df_modified[df_modified['year'] != 2020]\n","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:11:08.728856Z","iopub.execute_input":"2023-07-14T20:11:08.729663Z","iopub.status.idle":"2023-07-14T20:11:08.806920Z","shell.execute_reply.started":"2023-07-14T20:11:08.729626Z","shell.execute_reply":"2023-07-14T20:11:08.805756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_frame_style(df, caption=\"\"):\n    \"\"\"Helper function to set dataframe presentation style.\n    \"\"\"\n    return df.style.background_gradient(cmap='Blues').set_caption(caption).set_table_styles([{\n    'selector': 'caption',\n    'props': [\n        ('color', 'Blue'),\n        ('font-size', '18px'),\n        ('font-weight','bold')\n    ]}])\n\n\ndisplay(set_frame_style(df_modified.describe(),' Data : Summary Statistics'))","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:11:08.808801Z","iopub.execute_input":"2023-07-14T20:11:08.809277Z","iopub.status.idle":"2023-07-14T20:11:09.013461Z","shell.execute_reply.started":"2023-07-14T20:11:08.809232Z","shell.execute_reply":"2023-07-14T20:11:09.011792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separar o conjunto de treinamento e o conjunto de teste após o encoding\ndf_train = df_modified.loc[~df_modified['num_sold'].isna()]\ndf_test = df_modified.loc[df_modified['num_sold'].isna()]","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:11:09.015167Z","iopub.execute_input":"2023-07-14T20:11:09.016158Z","iopub.status.idle":"2023-07-14T20:11:09.031585Z","shell.execute_reply.started":"2023-07-14T20:11:09.016110Z","shell.execute_reply":"2023-07-14T20:11:09.030361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score\n\n\ncorr = df_train.corr(numeric_only=True)\ntarget_corr = corr['num_sold'].drop('num_sold')\n\n# Sort correlation values in descending order\ntarget_corr_sorted = target_corr.sort_values(ascending=False)\n\n# Create a heatmap of the correlations with the target column\nsns.set(font_scale=0.8)\nsns.set_style(\"white\")\nsns.set_palette(\"PuBuGn_d\")\nsns.heatmap(target_corr_sorted.to_frame(), cmap=\"coolwarm\", annot=True, fmt='.2f')\nplt.title('Correlation with Total Cup Points')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:11:09.034887Z","iopub.execute_input":"2023-07-14T20:11:09.035727Z","iopub.status.idle":"2023-07-14T20:11:10.309769Z","shell.execute_reply.started":"2023-07-14T20:11:09.035689Z","shell.execute_reply":"2023-07-14T20:11:10.308407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit\n\n\n\ndef objective(trial, df):\n    X = df.drop(['num_sold', 'id'], axis=1)\n    y = df['num_sold']\n    \n    # Definir o número de splits desejados\n    n_splits = 5\n\n    # Criar o objeto TimeSeriesSplit\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n\n    # Variável para armazenar a soma das métricas de cada fold\n    total_smape = 0\n\n    # Loop pelos splits do TimeSeriesSplit\n    for train_index, val_index in tscv.split(X):\n        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n\n        # Definir os hiperparâmetros a serem otimizados\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'max_depth': trial.suggest_int('max_depth', 3, 15),\n            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n            'random_state': 0\n        }\n\n        # Criar o modelo RandomForestRegressor com os hiperparâmetros definidos\n        model = RandomForestRegressor(**params)\n\n        # Treinar o modelo\n        model.fit(X_train, y_train)\n\n        # Prever os valores do conjunto de validação\n        y_pred = model.predict(X_val)\n\n        # Calcular a métrica de avaliação (SMAPE)\n        smape = calculate_smape(y_val, y_pred)\n\n        total_smape += smape\n\n    # Calcular a média das métricas dos folds\n    avg_smape = total_smape / n_splits\n\n    return avg_smape\n\n\n\ndef calculate_smape(y_true, y_pred):\n    return 1 / len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)) * 100)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:11:10.311660Z","iopub.execute_input":"2023-07-14T20:11:10.312768Z","iopub.status.idle":"2023-07-14T20:11:11.451984Z","shell.execute_reply.started":"2023-07-14T20:11:10.312719Z","shell.execute_reply":"2023-07-14T20:11:11.450608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_rf_regressor(df):\n    study = optuna.create_study(direction='minimize')\n    study.optimize(lambda trial: objective(trial, df), n_trials=3)\n\n    # Obter os melhores parâmetros encontrados\n    best_params = study.best_params\n    \n    X = df.drop('num_sold', axis=1)\n    y = df['num_sold']\n\n    # Criar o objeto TimeSeriesSplit\n    tscv = TimeSeriesSplit(n_splits=5)\n\n    # Lista para armazenar os modelos treinados\n    models = []\n\n    # Loop pelos splits do TimeSeriesSplit\n    for train_index, val_index in tscv.split(X):\n        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n\n        # Treinar um novo modelo usando os melhores parâmetros\n        model = RandomForestRegressor(**best_params)\n        model.fit(X_train, y_train)\n\n        models.append(model)\n\n    return models\n","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:11:11.453206Z","iopub.execute_input":"2023-07-14T20:11:11.454506Z","iopub.status.idle":"2023-07-14T20:11:11.468855Z","shell.execute_reply.started":"2023-07-14T20:11:11.454463Z","shell.execute_reply":"2023-07-14T20:11:11.465040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = train_rf_regressor(df_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:11:11.470360Z","iopub.execute_input":"2023-07-14T20:11:11.470722Z","iopub.status.idle":"2023-07-14T20:31:31.041624Z","shell.execute_reply.started":"2023-07-14T20:11:11.470693Z","shell.execute_reply":"2023-07-14T20:31:31.040337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.drop(['num_sold'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:31:31.043517Z","iopub.execute_input":"2023-07-14T20:31:31.044049Z","iopub.status.idle":"2023-07-14T20:31:31.052288Z","shell.execute_reply.started":"2023-07-14T20:31:31.043971Z","shell.execute_reply":"2023-07-14T20:31:31.050891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/playground-series-s3e19/sample_submission.csv')\n\n# Lista para armazenar as previsões de todos os modelos\nall_predictions = []\n\n# Loop pelos modelos treinados\nfor model in best_model:\n    # Fazer previsões para o modelo atual\n    predictions = model.predict(df_test)\n\n    # Armazenar as previsões do modelo atual na lista\n    all_predictions.append(predictions)\n\n# Calcular a média das previsões de todos os modelos\nmean_predictions = np.mean(all_predictions, axis=0)\n\n# Substituir a segunda coluna do sample_submission pela média das previsões\nsample_submission.iloc[:, 1] = mean_predictions\n\n# Salvar o DataFrame em um arquivo CSV\nsample_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T20:45:26.643919Z","iopub.execute_input":"2023-07-14T20:45:26.644389Z","iopub.status.idle":"2023-07-14T20:45:32.948186Z","shell.execute_reply.started":"2023-07-14T20:45:26.644351Z","shell.execute_reply":"2023-07-14T20:45:32.946847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}